# Wikipedia Crawler

Directory:

`LT6-Lab-APIs.ipynb`
- notebook demonstrating results of scraping a single Wikipedia page
- analyzes degrees of separation and distribution among multiple Wikipedia pages

`scraping_parallel.py`
- implementation of parallel processing of for loop for 1000 pages
- saves results of scraping `(starting page, degrees)` to a csv
- *CAUTION: works on MacOS M1, untested on PC*

`results_YYYYMMDD.csv`
- sample file for output of 1000-page crawler